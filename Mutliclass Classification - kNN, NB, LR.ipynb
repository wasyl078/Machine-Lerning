{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification\n",
    "### First method: k-nearest neighbours (kNN)\n",
    "### Second method: Naive Bayess\n",
    "### Third method: Logistic Regression + Gradient Descent + Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import sys\n",
    "import random\n",
    "import time\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Counting Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_time(f, param_1, param_2, name):\n",
    "    \"\"\"\n",
    "    This function counts how long does the given\n",
    "    function work, and also it returns the results\n",
    "    :param f: given function\n",
    "    :param param_1, param_2: parameters for the given function\n",
    "    :param name: Name of the functiob -> to display the time\n",
    "    :return: results of given function\n",
    "    \"\"\"\n",
    "    start = time.perf_counter()\n",
    "    res = f(param_1, param_2)\n",
    "    total = time.perf_counter() - start\n",
    "    print(f\"Time for: {name} => {total}s\")\n",
    "    return res\n",
    "\n",
    "def count_time_silent(f, param_1, param_2):\n",
    "    \"\"\"\n",
    "    This function also counts how long does\n",
    "    the given function work, but it only returns the time\n",
    "    :param f: given function\n",
    "    :param param_1, param_2: parameters for the given function\n",
    "    :return: value of time passed\n",
    "    \"\"\"\n",
    "    start = time.perf_counter()\n",
    "    _ = f(param_1, param_2)\n",
    "    total = time.perf_counter() - start\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generatar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_data_generator(N, noise):\n",
    "    \"\"\"\n",
    "    Generates data for my plots - a little bit random\n",
    "    with vissible class division\n",
    "    :param N: number of examples in each class\n",
    "    :param noise: value of noise for data randomizer\n",
    "    :return: dictionary of numpy arrays for data_x1, data_x2 and labels\n",
    "    \"\"\"\n",
    "    labels = np.array([[1]]*N + [[2]]*N + [[3]]*N + [[4]]*N)\n",
    "    data = [random_data(1.5, noise, N), random_data(1.5, noise, N), \n",
    "            random_data(0.5, noise, N), random_data(1.5, noise, N),\n",
    "            random_data(0.5, noise, N), random_data(0.5, noise, N),\n",
    "            random_data(1.5, noise, N), random_data(0.5, noise, N)]\n",
    "    conc_data_x1 = np.concatenate([d for d in data[::2]])\n",
    "    conc_data_x2 = np.concatenate([d for d in data[1::2]])\n",
    "    return {'x1': conc_data_x1, 'x2':conc_data_x2, 'y': labels}\n",
    "    \n",
    "    \n",
    "def random_data(a, noise, N):\n",
    "    \"\"\"\n",
    "    Generates random data in given range (noise)\n",
    "    :param a: starting point for generator\n",
    "    :param N: number of examples to generate\n",
    "    :param noise: value of noise for data randomizer\n",
    "    :return: numpy array\n",
    "    \"\"\"\n",
    "    data = np.random.uniform(a - noise, a + noise, size = N)\n",
    "    return np.array([[d] for d in data])\n",
    "\n",
    "\n",
    "def totally_random_data_generator(N):\n",
    "    \"\"\"\n",
    "    Generates totally random data in hardcoded range\n",
    "    :param N: number of examples to generate\n",
    "    :return: dictionary of numpy arrays\n",
    "    \"\"\"\n",
    "    data_x1 = np.random.uniform(0.1, 1.9,size = N)\n",
    "    data_x2 = np.random.uniform(0.1, 1.9,size = N)\n",
    "    return {'x1': data_x1, 'x2': data_x2}\n",
    "\n",
    "\n",
    "def advanced_theta_generator(n, mini, maxi):\n",
    "    \"\"\"\n",
    "    Creates vector Theta with first values of parameters\n",
    "    These values are random\n",
    "    :param n: number of parameters\n",
    "    :param mini, maxi: range for parameters randomizer\n",
    "    :param value: default parameter value\n",
    "    :return: vector nx1\n",
    "    \"\"\"\n",
    "    return np.array([[random.uniform(mini,maxi)] for i in range(n)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def error_function(distribution, y_labels):\n",
    "    \"\"\"\n",
    "    Calculates error value for given distribution\n",
    "    and given class labels\n",
    "    :param distribution: given distribution\n",
    "    :param y_labels: given class labels\n",
    "    :return: error value - flaot\n",
    "    \"\"\"\n",
    "    distribution = np.fliplr(distribution)\n",
    "    maxs = np.argmax(distribution, axis=1)\n",
    "    uniq = np.unique(y_labels).shape[0]\n",
    "    res = np.count_nonzero(maxs != (uniq - y_labels.T))\n",
    "    return res/y_labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def knn(train_data, val_data):\n",
    "    \"\"\"\n",
    "    Starting method for k-Nearest Neighbours,\n",
    "    it just separates data into single numpy arrays\n",
    "    :param train_data: training data \n",
    "    :param val_data: validation data\n",
    "    :return: results of knn_model_selection function\n",
    "    \"\"\"\n",
    "    x1_train = train_data['x1']\n",
    "    x2_train = train_data['x2']\n",
    "    y_train = train_data['y']\n",
    "    x1_val = val_data['x1']\n",
    "    x2_val = val_data['x2']\n",
    "    y_val = val_data['y']\n",
    "    return knn_model_selection(x1_val, x2_val, y_val, x1_train, x2_train, y_train)\n",
    "\n",
    "\n",
    "def knn_model_selection(x1_val, x2_val, y_val, x1_train, x2_train, y_train):\n",
    "    \"\"\"\n",
    "    Firstly, it calculates distances between \n",
    "    val and training data sets, sorts them\n",
    "    and chooses best k (best model). Finally\n",
    "    predicted labels values are returned\n",
    "    :param x1_val, x2_val, y_val: separated validation data \n",
    "    :param x1_train, x2_train, y_train: separated trasining data\n",
    "    :return: minimal error, best k value, all errors,\n",
    "    all k values, predicted labels for validation data\n",
    "    \"\"\"\n",
    "    dists = calculate_distance(x1_val, x2_val, x1_train, x2_train)\n",
    "    srt = sort_distances(dists, y_train)\n",
    "    k_values = np.arange(1,x1_train.shape[0])\n",
    "    error_list = list()\n",
    "    labels_list = list()\n",
    "    for k in k_values:\n",
    "        distri, labels = calculate_distribution_knn(srt, k)\n",
    "        error = error_function(distri, y_val)\n",
    "        error_list.append(error)\n",
    "        labels_list.append(labels)\n",
    "    min_idx = error_list.index(min(error_list))\n",
    "    return min(error_list), k_values[min_idx], error_list, k_values, labels_list[min_idx]\n",
    "\n",
    "\n",
    "def calculate_distance(x1, x2, x1_train, x2_train):\n",
    "    \"\"\"\n",
    "    Calculates Hamming distance between val and train data\n",
    "    :param x1, x2: validation data \n",
    "    :param x1_train, x2_train: training data\n",
    "    :return: numpy array of calculated distances\n",
    "    \"\"\"\n",
    "    n1 = np.shape(x1)[0]\n",
    "    n2 = np.shape(x1_train)[0]\n",
    "    dists = np.array([((x1[v, 0] - x1_train)**2 + (x2[v, 0] - x2_train)**2)**(0.5) for v in range(n1)])    \n",
    "    return dists[:,:,0]\n",
    "\n",
    "\n",
    "def sort_distances(dists, y_labels):\n",
    "    \"\"\"\n",
    "    Sorts y_labels respectively to sorted dists\n",
    "    :param dists: distances \n",
    "    :param y_labels: class labels\n",
    "    :return: sorted y_labels\n",
    "    \"\"\"\n",
    "    return y_labels[dists.argsort(kind='mergesort')][:,:,0]\n",
    "\n",
    "\n",
    "def calculate_distribution_knn(y_labels, k):\n",
    "    \"\"\"\n",
    "    Calculates distribution for kNN\n",
    "    :param y_labels: class labels \n",
    "    :param k: k value\n",
    "    :return: calculated distribution and predicted labels\n",
    "    \"\"\"\n",
    "    m = np.unique(y_labels).shape[0]\n",
    "    distri = np.ones((y_labels.shape[0], m))\n",
    "    for mi in range(m):\n",
    "        buf = np.count_nonzero(y_labels[:,:k] == mi+1, axis=1)\n",
    "        distri[:,mi] = buf\n",
    "    distri = distri / np.sum(distri, axis=1).reshape((y_labels.shape[0],1))\n",
    "    labels = np.argmax(distri, axis = 1)\n",
    "    return distri, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayess - secret ingredient is gaussian distribution\n",
    "\n",
    "$Y =  \\begin{bmatrix}y^{(0)}\\\\y^{1)}\\\\...\\\\y^{(n-1)}\\end{bmatrix}, X_{1} =  \\begin{bmatrix}x^{(0)}_1\\\\x^{(1)}_1\\\\...\\\\x^{(n-1)}_1\\end{bmatrix}, X_{2} =  \\begin{bmatrix}x^{(0)}_2\\\\x^{(1)}_2\\\\...\\\\x^{(n-1)}_2\\end{bmatrix}$\n",
    "\n",
    "$\\mu_{x,y} =  \\frac{1}{n}  \\sum_{i:y^{(i)} = y}^{} x^{(i)}_i$\n",
    "\n",
    "$\\sigma^2_{x,y} = \\frac{1}{n} \\sum_{i:y^{(i)} = y}(x^{(i)} -  \\mu_{x,y})$\n",
    "\n",
    "$p(x_j | y) = \\frac{1}{\\sqrt{2\\pi \\sigma^2_{x_j,y}}} \\cdot exp-\\frac{1}{2}(\\frac{(x-\\mu_{x_j,y})^2}{\\sigma^2_{x_j,y}})$ for j = 0,...,n-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_prioi(y):\n",
    "    \"\"\"\n",
    "    Calculates probability of occurance of\n",
    "    each value in given array\n",
    "    :param y: given array \n",
    "    :return: array of probabilities\n",
    "    \"\"\"\n",
    "    N = np.shape(y)[0]\n",
    "    M = np.unique(y).shape[0]\n",
    "    return np.array([(N - np.count_nonzero(y - i)) / N for i in range(1,M+1)])\n",
    "\n",
    "\n",
    "def calculate_means(x1, x2, y, N_val):\n",
    "    \"\"\"\n",
    "    Calculates the arithmetic mean \n",
    "    for each unique label\n",
    "    :param x1, x2: given arrays with data_x\n",
    "    :param y: given labels\n",
    "    :param N_val: number of val examples\n",
    "    :return: replicated array of means\n",
    "    \"\"\"\n",
    "    N = np.shape(x1)[0]\n",
    "    M = np.unique(y).shape[0]\n",
    "    buf = [[np.mean(x1[np.nonzero(y == m+1)[0], :]), np.mean(x2[np.nonzero(y == m+1)[0], :])] for m in range(M)]\n",
    "    res_single = np.array(buf).T    \n",
    "    return np.tile(res_single, N_val).reshape((2,N_val,M))\n",
    "\n",
    "\n",
    "def calculate_variance(x1, x2, y, N_val):\n",
    "    \"\"\"\n",
    "    Calculates the variance \n",
    "    for each unique label\n",
    "    :param x1, x2: given arrays with data_x\n",
    "    :param y: given labels\n",
    "    :param N_val: number of val examples\n",
    "    :return: replicated array of variances\n",
    "    \"\"\"\n",
    "    N = np.shape(x1)[0]\n",
    "    M = np.unique(y).shape[0]\n",
    "    buf = [[np.var(x1[np.nonzero(y == m+1)[0], :]), np.var(x2[np.nonzero(y == m+1)[0], :])] for m in range(M)]\n",
    "    res_single = np.array(buf).T    \n",
    "    return np.tile(res_single, N_val).reshape((2,N_val,M))\n",
    "\n",
    "\n",
    "def gauss(x1, x2, mean, var):\n",
    "    \"\"\"\n",
    "    Calculates gauss distribution\n",
    "    :param x1, x2: given arrays with data_x\n",
    "    :param mean: calculated means\n",
    "    :param var: calculated variances\n",
    "    :return: array with gauss distribution\n",
    "    \"\"\"\n",
    "    x = np.concatenate((x1, x2), axis=1).T\n",
    "    _, N, M = np.shape(mean)\n",
    "    distribution = np.ones((2,N,M))\n",
    "    for m in range(M):\n",
    "        fst_prt = 1 / (2*math.pi*var[:,:,m])**(0.5)\n",
    "        powerr = -0.5*((x - mean[:,:,m])**2 / var[:,:,m])\n",
    "        snd_prt = np.power(math.e, powerr)\n",
    "        together = fst_prt*snd_prt\n",
    "        distribution[:,:,m] = together\n",
    "    return distribution\n",
    "\n",
    "\n",
    "def p_x_y_nb(guass_distribution):\n",
    "    \"\"\"\n",
    "    Calculates product of gauss distribution\n",
    "    :param guass_distribution: given arrays with distribution\n",
    "    :return: array with product\n",
    "    \"\"\"\n",
    "    return np.prod(guass_distribution,axis=0).T\n",
    "\n",
    "\n",
    "def calculate_distribution_nb(apriori, p_x_y):\n",
    "    \"\"\"\n",
    "    Calculates distribution for naive bayess\n",
    "    :param apriori: probability of each class / label\n",
    "    :param p_x_y: gauss distribution\n",
    "    :return: array with nb distribution\n",
    "    \"\"\"\n",
    "    M, N = p_x_y.shape\n",
    "    numerator =  p_x_y * np.reshape(apriori, (M,1))\n",
    "    denumerator = np.tile(np.sum(numerator,axis=0),M).reshape((M,N)) \n",
    "    return np.divide(numerator, denumerator)\n",
    "\n",
    "\n",
    "def naive_bayess(train_data, val_data):\n",
    "    \"\"\"\n",
    "    Starting method for Naive Bayess,\n",
    "    it separates data into single numpy arrays,\n",
    "    and then learns and predicts labels for val data\n",
    "    :param train_data: training data \n",
    "    :param val_data: validation data\n",
    "    :return: error value and predicted labels\n",
    "    \"\"\"\n",
    "    # data\n",
    "    x1_train = train_data['x1']\n",
    "    x2_train = train_data['x2']\n",
    "    y_train = train_data['y']\n",
    "    x1_val = val_data['x1']\n",
    "    x2_val = val_data['x2']\n",
    "    y_val = val_data['y']\n",
    "    N1 = x1_val.shape[0]\n",
    "    N2 = x1_train.shape[0]\n",
    "    # > learning\n",
    "    apri = a_prioi(y_train)\n",
    "    ms = calculate_means(x1_train, x2_train, y_train, N1)\n",
    "    vs = calculate_variance(x1_train, x2_train, y_train, N1)\n",
    "    # > calculating\n",
    "    gau = gauss(x1_val, x2_val, ms, vs)\n",
    "    p_x_y = p_x_y_nb(gau)\n",
    "    distri = calculate_distribution_nb(apri, p_x_y)\n",
    "    return error_function(distri.T, y_val), np.argmax(distri.T,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression + gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Calculates value of sigmoid function\n",
    "    :param z: given data - array or number \n",
    "    :return: sigmoid value\n",
    "    \"\"\"\n",
    "    return 1/(1+math.e**(-z))\n",
    "    \n",
    "    \n",
    "def cost_function(xs, ys, theta):\n",
    "    \"\"\"\n",
    "    Calculates cost / error value\n",
    "    :param xs: given array with data_x\n",
    "    :param ys: given labels\n",
    "    :param theta: given theta parameters\n",
    "    :return: cost value\n",
    "    \"\"\"\n",
    "    m = np.shape(xs)[0]\n",
    "    sig = sigmoid(xs @ theta)\n",
    "    sig = np.clip(sig, -15, 15)\n",
    "    return -1/(2*m)*np.sum(ys * np.log(sig) + (1 - ys) * np.log(1 - sig))\n",
    "\n",
    "\n",
    "def design_matrix_interpreter(xs, polynomial):\n",
    "    \"\"\"\n",
    "    Generates matrix of x values,\n",
    "    depending on given polynomial\n",
    "    :param xs: given array with data_x\n",
    "    :param polynomial: given polynomial description\n",
    "    :return: design matrix of data_x\n",
    "    \"\"\"\n",
    "    polynomial += \" \"\n",
    "    m = xs.shape[0]\n",
    "    res = np.zeros((m, 1))\n",
    "    buf = np.ones((m,1))\n",
    "    for char in polynomial:\n",
    "        if char == \"0\":\n",
    "            buf = np.multiply(buf,np.ones((m,1)))\n",
    "        elif char == \"1\":\n",
    "            buf = np.multiply(buf,xs[:, 0].reshape((m,1)))\n",
    "        elif char == \"2\":\n",
    "            buf = np.multiply(buf,xs[:, 1].reshape((m,1)) )\n",
    "        elif char == \" \":\n",
    "            res = np.concatenate((res, buf), axis=1)\n",
    "            buf = np.ones((m,1))\n",
    "    return res[:, 1:]\n",
    "\n",
    "\n",
    "def gradient_descent(xs, y, th, alpha, iterations, xv, yv):\n",
    "    \"\"\"\n",
    "    Calculates as best as it gets theta parameters using gradient descent\n",
    "    :param xs: given array with data_x\n",
    "    :param y: given labels\n",
    "    :param th: some default random theta parameters\n",
    "    :param alpha: value of single lerning \"step\"\n",
    "    :param iterations: number of iterations for gradient descent\n",
    "    :return: array of theta parameters\n",
    "    \"\"\"\n",
    "    m = xs.shape[0]\n",
    "    for ii in range(iterations):\n",
    "        hx = sigmoid(xs@th)\n",
    "        buf = (hx - y)*xs\n",
    "        buf = np.sum(buf, axis = 0)\n",
    "        th = th - alpha/m*buf.reshape(len(buf), -1)\n",
    "    return th, cost_function(xv, yv, th)\n",
    "\n",
    "\n",
    "def normal_equation(xs, y, xv, yv):\n",
    "    \"\"\"\n",
    "    Calculates the best theta parameters using normal equation\n",
    "    :param xs: given array with data_x\n",
    "    :param y: given labels\n",
    "    :return: array of theta parameters, cost value\n",
    "    \"\"\"\n",
    "    buf = np.linalg.inv(xs.T @ xs)\n",
    "    th = buf @ xs.T @ y\n",
    "    return th, cost_function(xv, yv, th)\n",
    "\n",
    "\n",
    "def separate_data(y_train, y_val):\n",
    "    \"\"\"\n",
    "    Separates the data into classes\n",
    "    :param y_train: given labels for training data\n",
    "    :param y_val: given labels for validation data\n",
    "    :return: list of separeted data\n",
    "    \"\"\"\n",
    "    M = np.unique(y_train).shape[0]\n",
    "    new_ys = list()\n",
    "    for m in range(M):\n",
    "        # train\n",
    "        indexs_t = np.nonzero(y_train == m+1)[0]\n",
    "        buf_t = np.zeros(np.shape(y_train))\n",
    "        buf_t[indexs_t, :] += 1\n",
    "        new_ys.append(buf_t)\n",
    "        # validation\n",
    "        indexs_v = np.nonzero(y_val == m+1)[0]\n",
    "        buf_v = np.zeros(np.shape(y_val))\n",
    "        buf_v[indexs_v, :] += 1\n",
    "        new_ys.append(buf_v)\n",
    "    return new_ys\n",
    "\n",
    "    \n",
    "def logistic_regression(train_data, val_data):\n",
    "    \"\"\"\n",
    "    Starting method for Logistic Regression,\n",
    "    it learns and predicts labels for val data\n",
    "    by using two methods: gradient descent and normal equation\n",
    "    :param train_data: training data \n",
    "    :param val_data: validation data\n",
    "    :return: error values and predicted labels for both methods\n",
    "    \"\"\"\n",
    "    # settings\n",
    "    polynomials = [\"1 2\", \"0 1 2\", \"0 1 2 12\", \"0 1 2 11 22\", \"0 1 2 12 11 22\", \"0 1 2 12 11 22 111 222\",\n",
    "                  \"0 1 2 12 11 22 122 111 222\", \"0 1 2 12 11 22 112 111 222\"]\n",
    "    alpha = 0.1\n",
    "    iters = 5000\n",
    "    new_ys = separate_data(train_data['y'], val_data['y'])\n",
    "    best_models_gd = list()\n",
    "    best_polys_gd = list()\n",
    "    errors_gd = list()\n",
    "    best_models_ne = list()\n",
    "    best_polys_ne = list()\n",
    "    errors_ne = list()\n",
    "    # call for each class\n",
    "    for ii in range(int(len(new_ys)/2)):\n",
    "        xt = np.concatenate((train_data['x1'], train_data['x2']), axis=1)\n",
    "        xv = np.concatenate((val_data['x1'], val_data['x2']), axis=1)\n",
    "        yt = new_ys[2*ii]\n",
    "        yv = new_ys[2*ii+1]\n",
    "        th_gd, poly_gd, th_ne, poly_ne = logistic_regression_model_selection(polynomials, alpha, iters, xt, yt, xv, yv)\n",
    "        best_models_gd.append(th_gd)\n",
    "        best_polys_gd.append(poly_gd)\n",
    "        best_models_ne.append(th_ne)\n",
    "        best_polys_ne.append(poly_ne)\n",
    "        errors_gd.append(cost_function(design_matrix_interpreter(xv, poly_gd), yv, th_gd))\n",
    "        errors_ne.append(cost_function(design_matrix_interpreter(xv, poly_ne), yv, th_ne))\n",
    "    labels_gd = logistic_regression_distribution(xv,best_models_gd, best_polys_gd, np.unique(train_data['y']).shape[0])\n",
    "    error_gd = np.sum(np.array(errors_gd))\n",
    "    labels_ne = logistic_regression_distribution(xv,best_models_ne, best_polys_ne, np.unique(train_data['y']).shape[0])\n",
    "    error_ne = np.sum(np.array(errors_ne))\n",
    "    return error_gd, labels_gd, error_ne, labels_ne\n",
    "    \n",
    "    \n",
    "def logistic_regression_model_selection(polynomials, alpha, iters, xt, yt, xv, yv):\n",
    "    \"\"\"\n",
    "    Calculates best parameters with gradient descent\n",
    "    and the best parameters with normal_equation.\n",
    "    It also chooses best polynomial for each method\n",
    "    :param polynomials: list of polynomials to choose from\n",
    "    :param alpha: step size for gradient descent \n",
    "    :param iters: number of iterations for gradient descent\n",
    "    :param xt: given training data_x\n",
    "    :param yt: given training data_y - labels\n",
    "    :return: caluclated thetas and best polynomials for both methods\n",
    "    \"\"\"\n",
    "    ths_gd = list()\n",
    "    ths_ne = list()\n",
    "    costs_gd = list()\n",
    "    costs_ne = list()\n",
    "    for poly in polynomials:\n",
    "        xs_mat = design_matrix_interpreter(xt, poly)\n",
    "        xvs_mat = design_matrix_interpreter(xv, poly)\n",
    "        th = advanced_theta_generator(xs_mat.shape[1], 0, 0.12)\n",
    "        th_gd, cost_gd = gradient_descent(xs_mat, yt, th, alpha, iters, xvs_mat, yv)\n",
    "        th_ne, cost_ne = normal_equation(xs_mat, yt, xvs_mat, yv)\n",
    "        ths_gd.append(th_gd)\n",
    "        costs_gd.append(cost_gd)\n",
    "        ths_ne.append(th_ne)\n",
    "        costs_ne.append(cost_ne)\n",
    "    idx_gd = costs_gd.index(min(costs_gd))\n",
    "    idx_ne = costs_ne.index(min(costs_ne))\n",
    "    return ths_gd[idx_gd], polynomials[idx_gd], ths_gd[idx_ne], polynomials[idx_ne]\n",
    "    \n",
    "    \n",
    "def logistic_regression_distribution(xv, best_models, best_polys, unique_values):\n",
    "    \"\"\"\n",
    "    Calculates best distribution (here: labels) \n",
    "    for given model (here: thetas) and polynomials\n",
    "    :param xv: given array with data x\n",
    "    :param best_models: calculated theta parameters \n",
    "    :param best_polys: choosen polynomial\n",
    "    :param unique_values: number of unique classes / labels\n",
    "    :return: predicted labels\n",
    "    \"\"\"\n",
    "    distri = np.ones((xv.shape[0], 1))\n",
    "    for ii in range(unique_values):\n",
    "        class_id = ii + 1\n",
    "        model = best_models[ii]\n",
    "        poly = best_polys[ii]\n",
    "        xvs = design_matrix_interpreter(xv, poly)\n",
    "        buf = sigmoid(xvs@model)\n",
    "        distri = np.concatenate((distri, buf), axis=1)\n",
    "    distri = distri[:, 1:]\n",
    "    return np.argmax(distri, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 7.154176571176928\n",
      "1 6.477373588039885\n",
      "2 3.865623378604718\n",
      "3 3.295739864684957\n",
      "4 3.2491841768351866\n",
      "5 3.2416974848749076\n",
      "6 3.2354560632603855\n",
      "7 3.2288922868515653\n",
      "8 3.221509300814756\n",
      "9 3.2129425679830224\n",
      "10 3.202856409227152\n",
      "11 3.1909762934872505\n",
      "12 3.177131560828469\n",
      "13 3.1612373604850443\n",
      "14 3.1432153486153247\n",
      "15 3.1229446607686366\n",
      "16 3.1003063401034563\n",
      "17 3.0752731945708356\n",
      "18 3.0479205140068752\n",
      "19 3.018303291211157\n",
      "20 2.9863526467495665\n",
      "21 2.951918279160219\n",
      "22 2.91488297502748\n",
      "23 2.8752586703763727\n",
      "24 2.8332343076140636\n",
      "25 2.7891791190314414\n",
      "26 2.7436060192830554\n",
      "27 2.6970833855720646\n",
      "28 2.6501240098980854\n",
      "29 2.6031184931220577\n",
      "30 2.5563321640429817\n",
      "31 2.5099290811236146\n",
      "32 2.4639973267201927\n",
      "33 2.418586427475875\n",
      "34 2.3737767344424063\n",
      "35 2.3297502129001497\n",
      "36 2.286754907197495\n",
      "37 2.2449582514157593\n",
      "38 2.2044035668451847\n",
      "39 2.165109161552194\n",
      "40 2.127115157671893\n",
      "41 2.090437659002903\n",
      "42 2.0550326406587187\n",
      "43 2.0208163437359814\n",
      "44 1.9877036087257627\n",
      "45 1.9556267934241773\n",
      "46 1.9245348146604446\n",
      "47 1.894385643841976\n",
      "48 1.8651405057309016\n",
      "49 1.8367611850508965\n",
      "50 1.8092091174480254\n",
      "51 1.7824451342993144\n",
      "52 1.7564294982563886\n",
      "53 1.731122178737041\n",
      "54 1.7064832948313935\n",
      "55 1.6824736480930096\n",
      "56 1.659055391311292\n",
      "57 1.6361930583087816\n",
      "58 1.6138553023163928\n",
      "59 1.592017617117118\n",
      "60 1.5706658307582326\n",
      "61 1.5497991065832197\n",
      "62 1.5294299963499098\n",
      "63 1.5095793084380502\n",
      "64 1.4902666088874423\n",
      "65 1.4715011791603665\n",
      "66 1.4532784320526577\n",
      "67 1.4355823322956098\n",
      "68 1.4183904748961425\n",
      "69 1.401678540688375\n",
      "70 1.3854229392996176\n",
      "71 1.369601921853896\n",
      "72 1.3541958111054284\n",
      "73 1.339186842106708\n",
      "74 1.3245588845533995\n",
      "75 1.3102971689013123\n",
      "76 1.29638806083463\n",
      "77 1.2828188938058347\n",
      "78 1.2695778552820143\n",
      "79 1.2566539169255928\n",
      "80 1.2440367964632821\n",
      "81 1.2317169371687475\n",
      "82 1.2196854893457496\n",
      "83 1.2079342777122335\n",
      "84 1.1964557404490157\n",
      "85 1.1852428309933367\n",
      "86 1.1742888824894775\n",
      "87 1.1635874455480115\n",
      "88 1.1531321194666102\n",
      "89 1.1429164018053855\n",
      "90 1.1329335790420227\n",
      "91 1.1231766724757424\n",
      "92 1.113638441673454\n",
      "93 1.1043114366555271\n",
      "94 1.0951880829003289\n",
      "95 1.0862607812418952\n",
      "96 1.0775220069964586\n",
      "97 1.0689643972845837\n",
      "98 1.060580820610485\n",
      "99 1.05236442703192\n",
      "100 1.0443086801643509\n",
      "101 1.0364073738346236\n",
      "102 1.0286546367398268\n",
      "103 1.021044928351434\n",
      "104 1.0135730288516345\n",
      "105 1.0062340253204813\n",
      "106 0.9990232958391063\n",
      "107 0.9919364926982785\n",
      "108 0.984969525522183\n",
      "109 0.9781185448308375\n",
      "110 0.9713799263563216\n",
      "111 0.9647502562825199\n",
      "112 0.9582263174784319\n",
      "113 0.9518050767296626\n",
      "114 0.9454836729310208\n",
      "115 0.9392594061777289\n",
      "116 0.9331297276787556\n",
      "117 0.9270922304092919\n",
      "118 0.9211446404180373\n",
      "119 0.9152848087070362\n",
      "120 0.9095107036061779\n",
      "121 0.903820403570761\n",
      "122 0.8982120903374682\n",
      "123 0.8926840423824118\n",
      "124 0.8872346286332473\n",
      "125 0.8818623023964108\n",
      "126 0.8765655954693411\n",
      "127 0.8713431124165045\n",
      "128 0.8661935249963517\n",
      "129 0.8611155667344701\n",
      "130 0.8561080276450986\n",
      "131 0.8511697491095167\n",
      "132 0.8462996189248659\n",
      "133 0.8414965665406826\n",
      "134 0.8367595585031511\n",
      "135 0.8320875941280931\n",
      "136 0.8274797014238296\n",
      "137 0.8229349332835616\n",
      "138 0.8184523639647858\n",
      "139 0.814031085869917\n",
      "140 0.8096702066383\n",
      "141 0.8053688465556392\n",
      "142 0.8011261362819659\n",
      "143 0.7969412148948722\n",
      "144 0.7928132282400573\n",
      "145 0.7887413275772707\n",
      "146 0.7847246685060416\n",
      "147 0.78076241015248\n",
      "148 0.7768537145961573\n",
      "149 0.772997746514257\n",
      "150 0.7691936730192084\n",
      "151 0.7654406636654544\n",
      "152 0.7617378906013845\n",
      "153 0.7580845288428658\n",
      "154 0.7544797566460328\n",
      "155 0.750922755958357\n",
      "156 0.7474127129285836\n",
      "157 0.7439488184580588\n",
      "158 0.7405302687777094\n",
      "159 0.7371562660370065\n",
      "160 0.7338260188930102\n",
      "161 0.7305387430894892\n",
      "162 0.727293662017854\n",
      "163 0.7240900072531069\n",
      "164 0.7209270190597016\n",
      "165 0.7178039468632089\n",
      "166 0.7147200496852056\n",
      "167 0.7116745965394995\n",
      "168 0.7086668667888899\n",
      "169 0.7056961504623418\n",
      "170 0.7027617485330653\n",
      "171 0.6998629731585545\n",
      "172 0.6969991478840818\n",
      "173 0.6941696078113426\n",
      "174 0.6913736997344812\n",
      "175 0.6886107822456227\n",
      "176 0.6858802258123807\n",
      "177 0.683181412829934\n",
      "178 0.6805137376500718\n",
      "179 0.6778766065899937\n",
      "180 0.6752694379233214\n",
      "181 0.6726916618558961\n",
      "182 0.6701427204889226\n",
      "183 0.6676220677717701\n",
      "184 0.6651291694467856\n",
      "185 0.6626635029882696\n",
      "186 0.6602245575376285\n",
      "187 0.6578118338365153\n",
      "188 0.655424844159586\n",
      "189 0.6530631122482655\n",
      "190 0.6507261732465982\n",
      "191 0.64841357364002\n",
      "192 0.6461248711975228\n",
      "193 0.6438596349173131\n",
      "194 0.6416174449757032\n",
      "195 0.639397892678543\n",
      "196 0.6372005804140634\n",
      "197 0.6350251216055814\n",
      "198 0.6328711406620235\n",
      "199 0.630738272923912\n",
      "200 0.6286261646019344\n",
      "201 0.6265344727049771\n",
      "202 0.6244628649541528\n",
      "203 0.6224110196793067\n",
      "204 0.620378625694239\n",
      "205 0.6183653821471027\n",
      "206 0.6163709983425976\n",
      "207 0.6143951935329824\n",
      "208 0.6124376966753979\n",
      "209 0.6104982461538397\n",
      "210 0.6085765894648387\n",
      "211 0.6066724828670006\n",
      "212 0.6047856909955109\n",
      "213 0.6029159864439754\n",
      "214 0.6010631493169554\n",
      "215 0.5992269667577247\n",
      "216 0.5974072324566282\n",
      "217 0.5956037461463328\n",
      "218 0.5938163130907356\n",
      "219 0.592044743574701\n",
      "220 0.5902888524018304\n",
      "221 0.5885484584073559\n",
      "222 0.5868233839927031\n",
      "223 0.5851134546876368\n",
      "224 0.5834184987450722\n",
      "225 0.5817383467725465\n",
      "226 0.580072831403242\n",
      "227 0.5784217870083711\n",
      "228 0.5767850494515439\n",
      "229 0.5751624558846845\n",
      "230 0.5735538445841436\n",
      "231 0.5719590548248523\n",
      "232 0.5703779267896391\n",
      "233 0.5688103015104288\n",
      "234 0.5672560208377169\n",
      "235 0.5657149274344895\n",
      "236 0.5641868647908054\n",
      "237 0.5626716772553213\n",
      "238 0.561169210080289\n",
      "239 0.559679309476649\n",
      "240 0.5582018226764558\n",
      "241 0.5567365979998719\n",
      "242 0.5552834849245434\n",
      "243 0.5538423341554524\n",
      "244 0.5524129976936021\n",
      "245 0.5509953289022992\n",
      "246 0.549589182569954\n",
      "247 0.5481944149686696\n",
      "248 0.5468108839080257\n",
      "249 0.5454384487836638\n",
      "250 0.5440769706204356\n",
      "251 0.5427263121099929\n",
      "252 0.5413863376428395\n",
      "253 0.5400569133348381\n",
      "254 0.5387379070483845\n",
      "255 0.5374291884083876\n",
      "256 0.5361306288132984\n",
      "257 0.5348421014414376\n",
      "258 0.5335634812528961\n",
      "259 0.5322946449873073\n",
      "260 0.5310354711578159\n",
      "261 0.5297858400415031\n",
      "262 0.5285456336666587\n",
      "263 0.5273147357971498\n",
      "264 0.5260930319142216\n",
      "265 0.5248804091960635\n",
      "266 0.5236767564953919\n",
      "267 0.5224819643153937\n",
      "268 0.5212959247842641\n",
      "269 0.5201185316286352\n",
      "270 0.518949680146131\n",
      "271 0.5177892671773128\n",
      "272 0.5166371910771966\n",
      "273 0.515493351686588\n",
      "274 0.5143576503033996\n",
      "275 0.5132299896540959\n",
      "276 0.5121102738654819\n",
      "277 0.5109984084368936\n",
      "278 0.5098943002129372\n",
      "279 0.5087978573568916\n",
      "280 0.5077089893248096\n",
      "281 0.5066276068403911\n",
      "282 0.505553621870723\n",
      "283 0.5044869476028119\n",
      "284 0.5034274984210745\n",
      "285 0.5023751898856292\n",
      "286 0.5013299387115393\n",
      "287 0.5002916627488653\n",
      "288 0.49926028096359787\n",
      "289 0.4982357134193895\n",
      "290 0.4972178812600645\n",
      "291 0.496206706692904\n",
      "292 0.4952021129725853\n",
      "293 0.4942040243858134\n",
      "294 0.4932123662365476\n",
      "295 0.4922270648318031\n",
      "296 0.49124804746796547\n",
      "297 0.4902752424175735\n",
      "298 0.48930857891654933\n",
      "299 0.4883479871517883\n",
      "300 0.48739339824911504\n",
      "301 0.48644474426153883\n",
      "302 0.48550195815777897\n",
      "303 0.484564973811037\n",
      "304 0.4836337259879551\n",
      "305 0.4827081503377851\n",
      "306 0.48178818338168583\n",
      "307 0.4808737625021723\n",
      "308 0.47996482593266565\n",
      "309 0.47906131274714264\n",
      "310 0.4781631628498838\n",
      "311 0.4772703169652562\n",
      "312 0.47638271662759724\n",
      "313 0.475500304171113\n",
      "314 0.4746230227198479\n",
      "315 0.47375081617768294\n",
      "316 0.47288362921836724\n",
      "317 0.4720214072755946\n",
      "318 0.47116409653310054\n",
      "319 0.47031164391480423\n",
      "320 0.46946399707497266\n",
      "321 0.46862110438844057\n",
      "322 0.4677829149408401\n",
      "323 0.46694937851889234\n",
      "324 0.4661204456007319\n",
      "325 0.46529606734628987\n",
      "326 0.46447619558771247\n",
      "327 0.46366078281984274\n",
      "328 0.46284978219075995\n",
      "329 0.4620431474923792\n",
      "330 0.46124083315111963\n",
      "331 0.4604427942186409\n",
      "332 0.45964898636265233\n",
      "333 0.4588593658578012\n",
      "334 0.4580738895766468\n",
      "335 0.45729251498070655\n",
      "336 0.4565152001115953\n",
      "337 0.4557419035822697\n",
      "338 0.45497258456834044\n",
      "339 0.45420720279949817\n",
      "340 0.4534457185510364\n",
      "341 0.45268809263545645\n",
      "342 0.45193428639421135\n",
      "343 0.4511842616895171\n",
      "344 0.450437980896284\n",
      "345 0.44969540689416454\n",
      "346 0.4489565030596935\n",
      "347 0.44822123325855284\n",
      "348 0.4474895618379233\n",
      "349 0.44676145361897635\n",
      "350 0.44603687388945346\n",
      "351 0.4453157883963736\n",
      "352 0.44459816333883184\n",
      "353 0.44388396536092284\n",
      "354 0.44317316154479475\n",
      "355 0.4424657194037609\n",
      "356 0.4417616068755807\n",
      "357 0.44106079231581335\n",
      "358 0.44036324449128494\n",
      "359 0.43966893257369827\n",
      "360 0.4389778261332937\n",
      "361 0.4382898951326674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362 0.4376051099206653\n",
      "363 0.4369234412264041\n",
      "364 0.4362448601533818\n",
      "365 0.4355693381736938\n",
      "366 0.43489684712235394\n",
      "367 0.43422735919172484\n",
      "368 0.4335608469260318\n",
      "369 0.43289728321598403\n",
      "370 0.43223664129350037\n",
      "371 0.4315788947265104\n",
      "372 0.4309240174138752\n",
      "373 0.4302719835803933\n",
      "374 0.4296227677718815\n",
      "375 0.42897634485037744\n",
      "376 0.42833268998940466\n",
      "377 0.4276917786693493\n",
      "378 0.4270535866729025\n",
      "379 0.426418090080601\n",
      "380 0.4257852652664571\n",
      "381 0.42515508889365283\n",
      "382 0.4245275379103403\n",
      "383 0.42390258954549953\n",
      "384 0.42328022130490617\n",
      "385 0.42266041096713014\n",
      "386 0.42204313657966713\n",
      "387 0.4214283764551053\n",
      "388 0.4208161091673781\n",
      "389 0.42020631354810256\n",
      "390 0.4195989686829753\n",
      "391 0.4189940539082375\n",
      "392 0.4183915488072319\n",
      "393 0.41779143320700307\n",
      "394 0.4171936871749821\n",
      "395 0.4165982910157325\n",
      "396 0.4160052252677597\n",
      "397 0.4154144707003922\n",
      "398 0.41482600831072375\n",
      "399 0.41423981932061105\n",
      "400 0.4136558851737485\n",
      "401 0.4130741875327908\n",
      "402 0.4124947082765372\n",
      "403 0.4119174294971921\n",
      "404 0.41134233349764837\n",
      "405 0.41076940278887014\n",
      "406 0.41019862008729724\n",
      "407 0.4096299683123328\n",
      "408 0.40906343058385675\n",
      "409 0.4084989902198253\n",
      "410 0.4079366307338934\n",
      "411 0.40737633583311483\n",
      "412 0.40681808941567094\n",
      "413 0.40626187556867016\n",
      "414 0.4057076785659854\n",
      "415 0.40515548286614084\n",
      "416 0.4046052731102452\n",
      "417 0.40405703411998556\n",
      "418 0.4035107508956467\n",
      "419 0.4029664086141939\n",
      "420 0.4024239926273867\n",
      "421 0.40188348845994004\n",
      "422 0.4013448818077352\n",
      "423 0.4008081585360624\n",
      "424 0.4002733046779038\n",
      "425 0.3997403064322612\n",
      "426 0.39920915016252234\n",
      "427 0.3986798223948541\n",
      "428 0.3981523098166468\n",
      "429 0.39762659927497473\n",
      "430 0.39710267777510555\n",
      "431 0.39658053247903324\n",
      "432 0.3960601507040459\n",
      "433 0.3955415199213146\n",
      "434 0.3950246277545212\n",
      "435 0.3945094619785028\n",
      "436 0.3939960105179205\n",
      "437 0.3934842614459634\n",
      "438 0.3929742029830498\n",
      "439 0.39246582349558073\n",
      "440 0.39195911149466917\n",
      "441 0.39145405563492525\n",
      "442 0.3909506447132283\n",
      "443 0.3904488676675181\n",
      "444 0.389948713575602\n",
      "445 0.38945017165395546\n",
      "446 0.3889532312565431\n",
      "447 0.388457881873628\n",
      "448 0.38796411313059187\n",
      "449 0.3874719147867503\n",
      "450 0.38698127673416566\n",
      "451 0.38649218899645327\n",
      "452 0.3860046417275722\n",
      "453 0.38551862521061847\n",
      "454 0.38503412985659474\n",
      "455 0.38455114620317227\n",
      "456 0.3840696649134261\n",
      "457 0.38358967677457\n",
      "458 0.38311117269663775\n",
      "459 0.38263414371117765\n",
      "460 0.3821585809698954\n",
      "461 0.38168447574328046\n",
      "462 0.38121181941919813\n",
      "463 0.3807406035014674\n",
      "464 0.3802708196083809\n",
      "465 0.37980245947121516\n",
      "466 0.3793355149326874\n",
      "467 0.37886997794540406\n",
      "468 0.37840584057024196\n",
      "469 0.37794309497471673\n",
      "470 0.3774817334313064\n",
      "471 0.3770217483157373\n",
      "472 0.3765631321052365\n",
      "473 0.37610587737675494\n",
      "474 0.37564997680513545\n",
      "475 0.375195423161273\n",
      "476 0.37474220931022556\n",
      "477 0.37429032820928776\n",
      "478 0.37383977290606485\n",
      "479 0.37339053653648013\n",
      "480 0.3729426123227793\n",
      "481 0.3724959935715138\n",
      "482 0.37205067367149364\n",
      "483 0.3716066460917229\n",
      "484 0.3711639043793276\n",
      "485 0.37072244215746863\n",
      "486 0.3702822531232406\n",
      "487 0.36984333104557665\n",
      "488 0.3694056697631427\n",
      "489 0.368969263182244\n",
      "490 0.36853410527472136\n",
      "491 0.3681001900758804\n",
      "492 0.36766751168241374\n",
      "493 0.3672360642503513\n",
      "494 0.3668058419930372\n",
      "495 0.36637683917911507\n",
      "496 0.3659490501305621\n",
      "497 0.3655224692207426\n",
      "498 0.365097090872502\n",
      "499 0.36467290955629844\n"
     ]
    }
   ],
   "source": [
    "def neural_network(train_data, val_data):\n",
    "    x1_train = train_data['x1']\n",
    "    x2_train = train_data['x2']\n",
    "    y_train = train_data['y']\n",
    "    x1_val = val_data['x1']\n",
    "    x2_val = val_data['x2']\n",
    "    y_val = val_data['y']\n",
    "\n",
    "    \n",
    "def feedforward(xs, thetas):\n",
    "    a = xs.T\n",
    "    for Theta in thetas:\n",
    "        a = np.concatenate((np.ones((1,a.shape[1])), a), axis=0)\n",
    "        a = Theta @ a\n",
    "        a = sigmoid(a)\n",
    "    return a\n",
    "    \n",
    "    \n",
    "def sigmoid_gradient(z):\n",
    "    gz = sigmoid(z)\n",
    "    return gz * (1 - gz)\n",
    "    \n",
    "    \n",
    "def random_initialization(shape):\n",
    "    rangee = 0.12\n",
    "    return np.random.uniform(low=-rangee, high=rangee, size=shape)\n",
    "    \n",
    "    \n",
    "def cost_function_neural_network(X, y, thetas, lambdaa):\n",
    "    hx = feedforward(X, thetas).T\n",
    "    m = hx.shape[0]\n",
    "    labels = np.unique(y).shape[0]\n",
    "    ys = np.zeros((y.shape[0], labels))\n",
    "    ys[np.arange(0,ys.shape[0]),y.flatten()] = 1\n",
    "    J = 0\n",
    "    for ii in range(m):\n",
    "        buf = np.sum(-ys[ii,:]@np.log(hx[ii,:]).T - (1-ys[ii,:])@np.log(1-hx[ii,:]).T)\n",
    "        J += buf\n",
    "    J = J/m\n",
    "    \n",
    "    reg2 = np.sum(thetas[1][:,1:]**2)\n",
    "    reg1 = np.sum(thetas[0][:,1:]**2)\n",
    "    reg = lambdaa/(2*m) * (reg1 + reg2)\n",
    "    J = J + reg\n",
    "    \n",
    "    Theta1_grad = np.zeros(thetas[0].shape)\n",
    "    Theta2_grad = np.zeros(thetas[1].shape)\n",
    "    for t in range(m):\n",
    "        a1 = X[t,:].reshape((X.shape[1], 1))\n",
    "        a2 = sigmoid(thetas[0] @ np.concatenate((np.ones((1,a1.shape[1])), a1), axis=0))\n",
    "        a3 = sigmoid(thetas[1] @ np.concatenate((np.ones((1,a2.shape[1])), a2), axis=0))\n",
    "        \n",
    "        \n",
    "        dk3 = a3 - ys[t,:].reshape(a3.shape)\n",
    "    \n",
    "        dk2 = (thetas[1].T @ dk3)[1:]*sigmoid_gradient(thetas[0]@np.concatenate((np.ones((1,a1.shape[1])), a1), axis=0))\n",
    "    \n",
    "        Theta2_grad = Theta2_grad + dk3@np.concatenate((np.ones((1,a2.shape[1])), a2), axis=0).T\n",
    "        Theta1_grad = Theta1_grad + dk2@np.concatenate((np.ones((1,a1.shape[1])), a1), axis=0).T\n",
    "    Theta1_grad = Theta1_grad/m\n",
    "    Theta2_grad = Theta2_grad/m\n",
    "    \n",
    "    Theta1_grad[:,1:] = Theta1_grad[:,1:] + lambdaa/m * thetas[0][:,1:]\n",
    "    Theta2_grad[:,1:] = Theta2_grad[:,1:] + lambdaa/m * thetas[1][:,1:]\n",
    "    \n",
    "    \n",
    "    return J, Theta1_grad, Theta2_grad\n",
    "    \n",
    "def back_propagation():\n",
    "    pass\n",
    "\n",
    "\n",
    "def prediction(distri):\n",
    "    return np.argmax(distri.T, axis=1)\n",
    "\n",
    "data = scipy.io.loadmat('data sets/neural_data.mat')\n",
    "weights = scipy.io.loadmat('data sets/neural_thetas.mat')\n",
    "\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "y = y-1\n",
    "\n",
    "Theta1 = weights['Theta1']\n",
    "Theta2 = weights['Theta2']\n",
    "\n",
    "hx = feedforward(X, [Theta1, Theta2])\n",
    "\n",
    "J, Theta1_grad, Theta2_grad = cost_function_neural_network(X, y, [Theta1, Theta2], 1)\n",
    "\n",
    "\n",
    "thetas = [random_initialization(Theta1.shape), random_initialization(Theta2.shape)]\n",
    "lambdaa = 0\n",
    "alpha = 2.1\n",
    "m = X.shape[0]\n",
    "m = 1\n",
    "for ii in range(500):\n",
    "    J, Theta1_grad, Theta2_grad = cost_function_neural_network(X, y, thetas, lambdaa)\n",
    "    print(ii, J)\n",
    "    thetas[0] = thetas[0]*(1 - alpha*lambdaa/m) - alpha/m * Theta1_grad\n",
    "    thetas[1] = thetas[1]*(1 - alpha*lambdaa/m) - alpha/m * Theta2_grad\n",
    "\n",
    "    #0.48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------- Plot #1 Training Data -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_data_only(data, i, N):\n",
    "    \"\"\"\n",
    "    Displays only training data\n",
    "    :param data: given data_x and labels\n",
    "    :param i: number of plot in figure \n",
    "    :param N: number of examples from each class\n",
    "    \"\"\"\n",
    "    # settings\n",
    "    sub = standard_plot_settings(i)\n",
    "    sub.set_title(\"Training Data\", fontsize=12)\n",
    "    \n",
    "    # training data with colors\n",
    "    sub.plot(data['x1'][:N],data['x2'][:N], 'o', color='red')\n",
    "    sub.plot(data['x1'][N:2*N],data['x2'][N:2*N], 'x', color='green')\n",
    "    sub.plot(data['x1'][2*N:3*N],data['x2'][2*N:3*N], '^', color = 'blue')\n",
    "    sub.plot(data['x1'][3*N:],data['x2'][3*N:], 'v', color = 'yellow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------- Plot #2 Validation Data -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_validation_data(data_t, data_v, i, N_t, N_v):\n",
    "    \"\"\"\n",
    "    Displays training data (gray crosses)\n",
    "    and predicted labels for validation data (colourful)\n",
    "    :param data_t: given training data\n",
    "    :param data_v: given validation data\n",
    "    :param i: number of plot in figure \n",
    "    :param N_t: number of examples from each class in training data\n",
    "    :param N_v: number of examples from each class in validation data\n",
    "    \"\"\"\n",
    "    sub = standard_plot_settings(i)\n",
    "    sub.set_title(\"Validation Data\", fontsize=12)\n",
    "    \n",
    "    # training data\n",
    "    sub.plot(data_t['x1'][:N_t],data_t['x2'][:N_t], 'x', color='gray', markersize = 5)\n",
    "    sub.plot(data_t['x1'][N_t:2*N_t],data_t['x2'][N_t:2*N_t], 'x', color='gray', markersize = 5)\n",
    "    sub.plot(data_t['x1'][2*N_t:3*N_t],data_t['x2'][2*N_t:3*N_t], 'x', color = 'gray', markersize = 5)\n",
    "    sub.plot(data_t['x1'][3*N_t:],data_t['x2'][3*N_t:], 'x', color = 'gray', markersize = 5)\n",
    "    \n",
    "    # validation data\n",
    "    sub.plot(data_v['x1'][:N_v],data_v['x2'][:N_v], 'o', color='red')\n",
    "    sub.plot(data_v['x1'][N_v:2*N_v],data_v['x2'][N_v:2*N_v], 'x', color='green', markersize = 10)\n",
    "    sub.plot(data_v['x1'][2*N_v:3*N_v],data_v['x2'][2*N_v:3*N_v], '^', color = 'blue')\n",
    "    sub.plot(data_v['x1'][3*N_v:],data_v['x2'][3*N_v:], 'v', color = 'yellow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------- Plot #3 and #4 kNN -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_knn(data_t, data_v, i):\n",
    "    \"\"\"\n",
    "    Displays error value for each k (knn)\n",
    "    and predicted labels for validation data\n",
    "    :param data_t: given training data\n",
    "    :param data_v: given validation data\n",
    "    :param i: number of plot in figure \n",
    "    \"\"\"\n",
    "    min_err, best_k, errs, k_values, labels = count_time(knn, data_t, data_v, \"kNN\")\n",
    "    sub = fig.add_subplot(4, 2, i)\n",
    "    plt.xlabel(\"*k* Values\", fontsize=10)\n",
    "    plt.ylabel(\"Error Values\", fontsize=10)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    sub.set_xlim(min(k_values), max(k_values))\n",
    "    sub.set_ylim(0, 1)\n",
    "    #sub.set_xticks(np.arange(x1_min, x1_max, period))\n",
    "    sub.set_yticks(np.arange(0, 1.01, 0.1))\n",
    "    sub.set_title(f\"kNN Error Function\\nMin. err. = {min_err}\\nBest *k* = {best_k}\", fontsize=12)\n",
    "    sub.plot(k_values, errs)   \n",
    "    plot_prediction_visualisation(labels, i+1, \"kNN\", data_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------- Plot #5 Naive Bayess -----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_naive_bayess(data_t, data_v, i):\n",
    "    \"\"\"\n",
    "    Displays predicted labels for validation data (Naive Bayess)\n",
    "    :param data_t: given training data\n",
    "    :param data_v: given validation data\n",
    "    :param i: number of plot in figure \n",
    "    \"\"\"\n",
    "    error, labels = count_time(naive_bayess, data_t, data_v, \"Naive Bayess\")\n",
    "    plot_prediction_visualisation(labels, i, \"Naive Bayess\", data_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------- Plot #6 Logistic Regression-----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_logistic_regression(data_t, data_v, i):\n",
    "    \"\"\"\n",
    "    Displays predicted labels for validation data,\n",
    "    first: calulated with gradient descent,\n",
    "    second: calculated with normal equation\n",
    "    :param data_t: given training data\n",
    "    :param data_v: given validation data\n",
    "    :param i: number of plot in figure \n",
    "    \"\"\"\n",
    "    error_gd, labels_gd, error_ne, labels_ne = count_time(logistic_regression, data_t, data_v, \"Logistic Regression\")\n",
    "    plot_prediction_visualisation(labels_gd, i, \"Logistic Regression - Gradient Descent\", data_v)\n",
    "    plot_prediction_visualisation(labels_ne, i+1, \"Logistic Regression - Normal Equation\", data_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------------------------------- Plots Displaying--------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_visualisation(labels, i, algo_name, data_v):\n",
    "    \"\"\"\n",
    "    Sets all standard settings for plot,\n",
    "    and displays predicted labels for validation data \n",
    "    and algo / method name\n",
    "    :param labels: given labels\n",
    "    :param i: number of plot in figure\n",
    "    :param algo_name: name of method to display\n",
    "    :param data_v: given validation data\n",
    "    \"\"\"\n",
    "    red = list(), list()\n",
    "    green = list(), list()\n",
    "    blue = list(), list()\n",
    "    yellow = list(), list()\n",
    "    x1_val = data_v['x1']\n",
    "    x2_val = data_v['x2']\n",
    "    y_val = data_v['y']\n",
    "    N = x1_val.shape[0]\n",
    "    for n in range(N):\n",
    "        if labels[n] == 0:\n",
    "            red[0].append(x1_val[n,0])\n",
    "            red[1].append(x2_val[n,0])\n",
    "        elif labels[n] == 1:\n",
    "            green[0].append(x1_val[n,0])\n",
    "            green[1].append(x2_val[n,0])\n",
    "        elif labels[n] == 2:\n",
    "            blue[0].append(x1_val[n,0])\n",
    "            blue[1].append(x2_val[n,0])\n",
    "        elif labels[n] == 3:\n",
    "            yellow[0].append(x1_val[n,0])\n",
    "            yellow[1].append(x2_val[n,0])\n",
    "\n",
    "    sub = standard_plot_settings(i)\n",
    "    \n",
    "    # val data\n",
    "    sub.plot(np.array(red[0]), np.array(red[1]), 'o', color='red')\n",
    "    sub.plot(np.array(green[0]), np.array(green[1]), 'x', color='green')\n",
    "    sub.plot(np.array(blue[0]), np.array(blue[1]), '^', color = 'blue')\n",
    "    sub.plot(np.array(yellow[0]), np.array(yellow[1]), 'v', color = 'yellow') \n",
    "\n",
    "    # displaying the ones that were wrong predicted\n",
    "    true_ys = data_v[\"y\"]\n",
    "    all_xs = np.concatenate((data_v[\"x1\"], data_v[\"x2\"]), axis=1)\n",
    "    indexes = np.nonzero(labels != (true_ys.T-1))[1]\n",
    "    diff = all_xs[indexes, :]\n",
    "    sub.plot(diff[:,0], diff[:,1], 'o', color = 'black',fillstyle='none',markersize=20)\n",
    "    \n",
    "    prec = (1 - diff.shape[0] / labels.shape[0]) * 100\n",
    "    sub.set_title(f\"{algo_name}\\nPrecision: {prec}%\", fontsize=12)\n",
    "    \n",
    "    \n",
    "def standard_plot_settings(i):\n",
    "    \"\"\"\n",
    "    Sets all standard settings for plot\n",
    "    :param i: number of plot in figure\n",
    "    :return: subplot\n",
    "    \"\"\"\n",
    "    x1_min = 0\n",
    "    x1_max = 2.01\n",
    "    x2_min = 0\n",
    "    x2_max = 2.01\n",
    "    period = 0.1\n",
    "    sub = fig.add_subplot(4, 2, i)\n",
    "    plt.xlabel(\"X1 Axis\", fontsize=10)\n",
    "    plt.ylabel(\"X2 Axis\", fontsize=10)\n",
    "    plt.xticks(fontsize=10)\n",
    "    plt.yticks(fontsize=10)\n",
    "    sub.set_xlim(x1_min, x1_max)\n",
    "    sub.set_ylim(x2_min, x2_max)\n",
    "    sub.set_xticks(np.arange(x1_min, x1_max, period))\n",
    "    sub.set_yticks(np.arange(x2_min, x2_max, period))\n",
    "    return sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------------------------------------------- All Plots -------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N_v = 20\n",
    "N_t = 120\n",
    "training_data = standard_data_generator(N_t, 0.45)\n",
    "validation_data = standard_data_generator(N_v, 0.55)\n",
    "fig = plt.figure(figsize=(15, 25))\n",
    "plot_training_data_only(training_data, 1, N_t)\n",
    "plot_validation_data(training_data, validation_data, 2, N_t, N_v)\n",
    "plot_knn(training_data, validation_data, 3)\n",
    "plot_naive_bayess(training_data, validation_data, 5)\n",
    "plot_logistic_regression(training_data, validation_data, 6)\n",
    "plt.tight_layout()\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
